{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGovGFQrDtCh"
      },
      "outputs": [],
      "source": [
        "#@title Install the client libraries\n",
        "!pip install --upgrade google-cloud-videointelligence\n",
        "!pip install --upgrade google-auth\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Import the required modules\n",
        "from google.cloud import storage\n",
        "from google.cloud import videointelligence\n",
        "from google.colab import auth\n",
        "\n",
        "from vertexai.preview.language_models import TextGenerationModel\n",
        "import io\n",
        "import json\n",
        "import time\n"
      ],
      "metadata": {
        "id": "PiAJKXRQGEXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Specify your Google Cloud Project id & Authenticate\n",
        "project_id='INSERT-YOUR-PROJECT-ID'\n",
        "auth.authenticate_user(project_id=project_id)"
      ],
      "metadata": {
        "id": "Ovcu5_OXGHy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Parameters\n",
        "input_bucket_name = \"abcd_input_bucket\" #@param\n",
        "output_bucket_name = \"abcd_output_bucket\" #@param\n",
        "speech_language_code = \"en-EN\" #@param\n",
        "\n",
        "#change input & output\n",
        "\n",
        "# thresholds\n",
        "confidence_threshold = 0.6 #@param\n",
        "early_time_seconds = 5 #@param\n",
        "\n",
        "# face early\n",
        "duration_threshold_seconds = 1 #@param\n",
        "surface_threshold_percent = 0.05 #@param\n",
        "\n",
        "# pacing quick\n",
        "avg_shot_duration_seconds = 2 #@param\n",
        "\n",
        "# dynamic start\n",
        "dynamic_cutoff_ms = 3000  #@param\n",
        "changed_pixels_minimum = 30  # param\n",
        "\n",
        "# logo big & early, name early\n",
        "logo_size_threshold = 3.5  #@param\n",
        "# Opaque entity ID. Some IDs may be available in [Google Knowledge Graph\n",
        "# Search API](https://developers.google.com/knowledge-graph/).\n",
        "entity_id = \"/g/11cm05x9g0\" #@param\n",
        "entity_desc = \"Pixel\"\n",
        "brand_name = \"Pixel\" #@param\n",
        "\n",
        "# product early\n",
        "duration_threshold = 1\n",
        "# Opaque entity ID. Some IDs may be available in [Google Knowledge Graph\n",
        "# Search API](https://developers.google.com/knowledge-graph/).\n",
        "product_entity_id = None #@param\n",
        "product_entity_desc = \"television advertisement\" #@param\n",
        "\n",
        "# face close\n",
        "face_duration_threshold_seconds = 1 #@param\n",
        "face_surface_threshold = 0.05 #@param\n",
        "\n",
        "# overall pacing\n",
        "overall_shot_pace_threshold = 2 #@param\n",
        "\n",
        "#clients\n",
        "storage_client = storage.Client()\n",
        "input_bucket = storage_client.get_bucket(input_bucket_name)\n",
        "output_bucket = storage_client.get_bucket(output_bucket_name)\n",
        "\n",
        "video_client = videointelligence.VideoIntelligenceServiceClient()"
      ],
      "metadata": {
        "id": "yHhFxalekgLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 1) Label Detection: API Request\n",
        "def label_detection_api_request(video_name, gs_uri):\n",
        "  \"\"\"Detects labels in a video.\"\"\"\n",
        "  filename = f\"labeldetection/labeldetection-{video_name}.json\"\n",
        "  videos = storage_client.list_blobs(output_bucket_name)\n",
        "\n",
        "  if(videos):\n",
        "    for v in videos:\n",
        "      if(v.name == filename):\n",
        "        # print(f\"File already exists: {filename} not sending API request\")\n",
        "        return\n",
        "\n",
        "  output_uri = f\"gs://{output_bucket_name}/{filename}\"\n",
        "\n",
        "\n",
        "  features = [videointelligence.Feature.LABEL_DETECTION]\n",
        "  operation = video_client.annotate_video(\n",
        "      request={\n",
        "          \"features\": features,\n",
        "          \"input_uri\": gs_uri,\n",
        "          \"output_uri\": filename,\n",
        "      }\n",
        "  )\n",
        "  print(\"\\nProcessing video for label annotations:\")\n",
        "\n",
        "  result = operation.result(timeout=180)\n",
        "\n",
        "  # first result is retrieved because a single video was processed\n",
        "  segment_labels = result.annotation_results[0].shot_label_annotations\n",
        "\n",
        "  for i, segment_label in enumerate(segment_labels):\n",
        "      print(\"Video label description: {}\".format(segment_label.entity.description))\n",
        "      for category_entity in segment_label.category_entities:\n",
        "          print(\n",
        "              \"\\tLabel category description: {}\".format(category_entity.description)\n",
        "          )\n",
        "\n",
        "      for i, segment in enumerate(segment_label.segments):\n",
        "          start_time = (\n",
        "              segment.segment.start_time_offset.seconds\n",
        "              + segment.segment.start_time_offset.microseconds / 1e6\n",
        "          )\n",
        "          end_time = (\n",
        "              segment.segment.end_time_offset.seconds\n",
        "              + segment.segment.end_time_offset.microseconds / 1e6\n",
        "          )\n",
        "          positions = \"{}s to {}s\".format(start_time, end_time)\n",
        "          confidence = segment.confidence\n",
        "          print(\"\\tSegment {}: {}\".format(i, positions))\n",
        "          print(\"\\tConfidence: {}\".format(confidence))\n",
        "      print(\"\\n\")"
      ],
      "metadata": {
        "id": "tFruroGKD-aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 2) Detect Faces: API Request\n",
        "def faces_detection_api_request(video_name, gs_uri):\n",
        "    \"\"\"Detects faces in a video.\"\"\"\n",
        "    filename = f\"facesdetection/facesdetection-{video_name}.json\"\n",
        "    videos = storage_client.list_blobs(output_bucket_name)\n",
        "\n",
        "    if(videos):\n",
        "      for v in videos:\n",
        "        if(v.name == filename):\n",
        "          # print(f\"File already exists: {filename} not sending API request\")\n",
        "          return\n",
        "\n",
        "    output_uri = f\"gs://{output_bucket_name}/{filename}\"\n",
        "\n",
        "    # Configure the request\n",
        "    config = videointelligence.FaceDetectionConfig(\n",
        "        include_bounding_boxes=True, include_attributes=True\n",
        "    )\n",
        "    context = videointelligence.VideoContext(face_detection_config=config)\n",
        "\n",
        "    # Start the asynchronous request\n",
        "    operation = video_client.annotate_video(\n",
        "        request={\n",
        "            \"features\": [videointelligence.Feature.FACE_DETECTION],\n",
        "            \"input_uri\": gs_uri,\n",
        "            \"output_uri\": output_uri,\n",
        "            \"video_context\": context,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(\"\\nProcessing video for face detection annotations.\")\n",
        "    result = operation.result(timeout=300)\n",
        "\n",
        "    print(\"\\nFinished processing.\\n\")\n",
        "\n",
        "    # Retrieve the first result, because a single video was processed.\n",
        "    annotation_result = result.annotation_results[0]\n",
        "\n",
        "    for index, annotation in enumerate(annotation_result.face_detection_annotations):\n",
        "        print(\"Face detected: \" + format(index))\n",
        "        for track in annotation.tracks:\n",
        "            print(\n",
        "                \"Segment: {}s to {}s\".format(\n",
        "                    track.segment.start_time_offset.seconds\n",
        "                    + track.segment.start_time_offset.microseconds / 1e6,\n",
        "                    track.segment.end_time_offset.seconds\n",
        "                    + track.segment.end_time_offset.microseconds / 1e6,\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Each segment includes timestamped faces that include\n",
        "            # characteristics of the face detected.\n",
        "            # Grab the first timestamped face\n",
        "            timestamped_object = track.timestamped_objects[0]\n",
        "            box = timestamped_object.normalized_bounding_box\n",
        "            print(\"Bounding box:\")\n",
        "            print(\"\\tleft  : {}\".format(box.left))\n",
        "            print(\"\\ttop   : {}\".format(box.top))\n",
        "            print(\"\\tright : {}\".format(box.right))\n",
        "            print(\"\\tbottom: {}\".format(box.bottom))\n",
        "\n",
        "            # Attributes include glasses, headwear, smiling, direction of gaze\n",
        "            print(\"Attributes:\")\n",
        "            for attribute in timestamped_object.attributes:\n",
        "                print(\n",
        "                    \"\\t{}:{} {}\".format(\n",
        "                        attribute.name, attribute.value, attribute.confidence\n",
        "                    )\n",
        "                )"
      ],
      "metadata": {
        "id": "QpMePQKfHAsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 3) Shot Detection: API Request\n",
        "def shot_detection_api_request(video_name, gs_uri):\n",
        "  filename = f\"shotdetection/shotdetection-{video_name}.json\"\n",
        "  videos = storage_client.list_blobs(output_bucket_name)\n",
        "\n",
        "  if(videos):\n",
        "    for v in videos:\n",
        "      if(v.name == filename):\n",
        "        # print(f\"File already exists: {filename} not sending API request\")\n",
        "        return\n",
        "\n",
        "  output_uri = f\"gs://{output_bucket_name}/{filename}\"\n",
        "\n",
        "  \"\"\"Detects camera shot changes.\"\"\"\n",
        "  features = [videointelligence.Feature.SHOT_CHANGE_DETECTION]\n",
        "  operation = video_client.annotate_video(\n",
        "      request={\"features\": features,\n",
        "              \"input_uri\": gs_uri,\n",
        "              \"output_uri\": output_uri}\n",
        "  )\n",
        "  print(\"\\nProcessing video for shot change annotations:\")\n",
        "\n",
        "  result = operation.result(timeout=90)\n",
        "  print(\"\\nFinished processing.\")\n",
        "\n",
        "  # first result is retrieved because a single video was processed\n",
        "  for i, shot in enumerate(result.annotation_results[0].shot_annotations):\n",
        "      start_time = (\n",
        "          shot.start_time_offset.seconds + shot.start_time_offset.microseconds / 1e6\n",
        "      )\n",
        "      end_time = (\n",
        "          shot.end_time_offset.seconds + shot.end_time_offset.microseconds / 1e6\n",
        "      )\n",
        "      print(\"\\tShot {}: {} to {}\".format(i, start_time, end_time))\n",
        "\n"
      ],
      "metadata": {
        "id": "VjgC-norHQyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 4) Detect Text: API Request\n",
        "def text_detection_api_request(video_name, gs_uri):\n",
        "  filename = f\"textdetection/textdetection-{video_name}.json\"\n",
        "  videos = storage_client.list_blobs(output_bucket_name)\n",
        "\n",
        "  if(videos):\n",
        "    for v in videos:\n",
        "      if(v.name == filename):\n",
        "        # print(f\"File already exists: {filename} not sending API request\")\n",
        "        return\n",
        "\n",
        "  output_uri = f\"gs://{output_bucket_name}/{filename}\"\n",
        "\n",
        "  features = [videointelligence.Feature.TEXT_DETECTION]\n",
        "\n",
        "  operation = video_client.annotate_video(\n",
        "      request={\"features\": features,\n",
        "               \"input_uri\": gs_uri,\n",
        "               \"output_uri\": output_uri}\n",
        "  )\n",
        "\n",
        "  print(\"\\nProcessing video for text detection.\")\n",
        "  result = operation.result(timeout=600)\n",
        "\n",
        "  # The first result is retrieved because a single video was processed.\n",
        "  annotation_result = result.annotation_results[0]\n",
        "\n",
        "  for text_annotation in annotation_result.text_annotations:\n",
        "      print(\"\\nText: {}\".format(text_annotation.text))\n",
        "\n",
        "      # Get the first text segment\n",
        "      text_segment = text_annotation.segments[0]\n",
        "      start_time = text_segment.segment.start_time_offset\n",
        "      end_time = text_segment.segment.end_time_offset\n",
        "      print(\n",
        "          \"start_time: {}, end_time: {}\".format(\n",
        "              start_time.seconds + start_time.microseconds * 1e-6,\n",
        "              end_time.seconds + end_time.microseconds * 1e-6,\n",
        "          )\n",
        "      )\n",
        "\n",
        "      print(\"Confidence: {}\".format(text_segment.confidence))\n",
        "\n",
        "      # Show the result for the first frame in this segment.\n",
        "      frame = text_segment.frames[0]\n",
        "      time_offset = frame.time_offset\n",
        "      print(\n",
        "          \"Time offset for the first frame: {}\".format(\n",
        "              time_offset.seconds + time_offset.microseconds * 1e-6\n",
        "          )\n",
        "      )\n",
        "      print(\"Rotated Bounding Box Vertices:\")\n",
        "      for vertex in frame.rotated_bounding_box.vertices:\n",
        "          print(\"\\tVertex.x: {}, Vertex.y: {}\".format(vertex.x, vertex.y))"
      ],
      "metadata": {
        "id": "Ft1xuam6KHIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 5) Logo Detection: API Request\n",
        "def logo_detection_api_request(video_name, gs_uri):\n",
        "    filename = f\"logodetection/logodetection-{video_name}.json\"\n",
        "    videos = storage_client.list_blobs(output_bucket_name)\n",
        "\n",
        "    if(videos):\n",
        "      for v in videos:\n",
        "        if(v.name == filename):\n",
        "          # print(f\"File already exists: {filename} not sending API request\")\n",
        "          return\n",
        "\n",
        "    output_uri = f\"gs://{output_bucket_name}/{filename}\"\n",
        "\n",
        "    features = [videointelligence.Feature.LOGO_RECOGNITION]\n",
        "\n",
        "    operation = video_client.annotate_video(\n",
        "        request={\"features\": features,\n",
        "                 \"input_uri\": gs_uri,\n",
        "                 \"output_uri\": output_uri}\n",
        "    )\n",
        "\n",
        "    print(\"Waiting for operation to complete...\")\n",
        "    response = operation.result()\n",
        "\n",
        "    # Get the first response, since we sent only one video.\n",
        "    annotation_result = response.annotation_results[0]\n",
        "\n",
        "    # Annotations for list of logos detected, tracked and recognized in video.\n",
        "    for logo_recognition_annotation in annotation_result.logo_recognition_annotations:\n",
        "        entity = logo_recognition_annotation.entity\n",
        "\n",
        "        # Opaque entity ID. Some IDs may be available in [Google Knowledge Graph\n",
        "        # Search API](https://developers.google.com/knowledge-graph/).\n",
        "        print(\"Entity Id : {}\".format(entity.entity_id))\n",
        "\n",
        "        print(\"Description : {}\".format(entity.description))\n",
        "\n",
        "        # All logo tracks where the recognized logo appears. Each track corresponds\n",
        "        # to one logo instance appearing in consecutive frames.\n",
        "        for track in logo_recognition_annotation.tracks:\n",
        "            # Video segment of a track.\n",
        "            print(\n",
        "                \"\\n\\tStart Time Offset : {}.{}\".format(\n",
        "                    track.segment.start_time_offset.seconds,\n",
        "                    track.segment.start_time_offset.microseconds * 1000,\n",
        "                )\n",
        "            )\n",
        "            print(\n",
        "                \"\\tEnd Time Offset : {}.{}\".format(\n",
        "                    track.segment.end_time_offset.seconds,\n",
        "                    track.segment.end_time_offset.microseconds * 1000,\n",
        "                )\n",
        "            )\n",
        "            print(\"\\tConfidence : {}\".format(track.confidence))\n",
        "\n",
        "            # The object with timestamp and attributes per frame in the track.\n",
        "            for timestamped_object in track.timestamped_objects:\n",
        "                # Normalized Bounding box in a frame, where the object is located.\n",
        "                normalized_bounding_box = timestamped_object.normalized_bounding_box\n",
        "                print(\"\\n\\t\\tLeft : {}\".format(normalized_bounding_box.left))\n",
        "                print(\"\\t\\tTop : {}\".format(normalized_bounding_box.top))\n",
        "                print(\"\\t\\tRight : {}\".format(normalized_bounding_box.right))\n",
        "                print(\"\\t\\tBottom : {}\".format(normalized_bounding_box.bottom))\n",
        "\n",
        "                # Optional. The attributes of the object in the bounding box.\n",
        "                for attribute in timestamped_object.attributes:\n",
        "                    print(\"\\n\\t\\t\\tName : {}\".format(attribute.name))\n",
        "                    print(\"\\t\\t\\tConfidence : {}\".format(attribute.confidence))\n",
        "                    print(\"\\t\\t\\tValue : {}\".format(attribute.value))\n",
        "\n",
        "            # Optional. Attributes in the track level.\n",
        "            for track_attribute in track.attributes:\n",
        "                print(\"\\n\\t\\tName : {}\".format(track_attribute.name))\n",
        "                print(\"\\t\\tConfidence : {}\".format(track_attribute.confidence))\n",
        "                print(\"\\t\\tValue : {}\".format(track_attribute.value))\n",
        "\n",
        "        # All video segments where the recognized logo appears. There might be\n",
        "        # multiple instances of the same logo class appearing in one VideoSegment.\n",
        "        for segment in logo_recognition_annotation.segments:\n",
        "            print(\n",
        "                \"\\n\\tStart Time Offset : {}.{}\".format(\n",
        "                    segment.start_time_offset.seconds,\n",
        "                    segment.start_time_offset.microseconds * 1000,\n",
        "                )\n",
        "            )\n",
        "            print(\n",
        "                \"\\tEnd Time Offset : {}.{}\".format(\n",
        "                    segment.end_time_offset.seconds,\n",
        "                    segment.end_time_offset.microseconds * 1000,\n",
        "                )\n",
        "            )"
      ],
      "metadata": {
        "id": "aMsZVL-BKrci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 6) Transcribe the video: API Request\n",
        "def speech_detection_api_request(video_name, gs_uri):\n",
        "  filename = f\"transcribe/transcribe-{video_name}.json\"\n",
        "  videos = storage_client.list_blobs(output_bucket_name)\n",
        "\n",
        "  if(videos):\n",
        "    for v in videos:\n",
        "      if(v.name == filename):\n",
        "        # print(f\"File already exists: {filename} not sending API request\")\n",
        "        return\n",
        "\n",
        "  output_uri = f\"gs://{output_bucket_name}/{filename}\"\n",
        "\n",
        "  features = [videointelligence.Feature.SPEECH_TRANSCRIPTION]\n",
        "\n",
        "  config = videointelligence.SpeechTranscriptionConfig(\n",
        "      language_code=speech_language_code, enable_automatic_punctuation=True\n",
        "  )\n",
        "  video_context = videointelligence.VideoContext(speech_transcription_config=config)\n",
        "\n",
        "  operation = video_client.annotate_video(\n",
        "      request={\n",
        "          \"features\": features,\n",
        "          \"input_uri\": gs_uri,\n",
        "          \"output_uri\": output_uri,\n",
        "          \"video_context\": video_context,\n",
        "      }\n",
        "  )\n",
        "\n",
        "  print(\"\\nProcessing video for speech transcription.\")\n",
        "\n",
        "  result = operation.result(timeout=600)\n",
        "\n",
        "  # There is only one annotation_result since only\n",
        "  # one video is processed.\n",
        "  annotation_results = result.annotation_results[0]\n",
        "  for speech_transcription in annotation_results.speech_transcriptions:\n",
        "      # The number of alternatives for each transcription is limited by\n",
        "      # SpeechTranscriptionConfig.max_alternatives.\n",
        "      # Each alternative is a different possible transcription\n",
        "      # and has its own confidence score.\n",
        "      for alternative in speech_transcription.alternatives:\n",
        "          print(\"Alternative level information:\")\n",
        "\n",
        "          print(\"Transcript: {}\".format(alternative.transcript))\n",
        "          print(\"Confidence: {}\\n\".format(alternative.confidence))\n",
        "\n",
        "          print(\"Word level information:\")\n",
        "          for word_info in alternative.words:\n",
        "              word = word_info.word\n",
        "              start_time = word_info.start_time\n",
        "              end_time = word_info.end_time\n",
        "              print(\n",
        "                  \"\\t{}s - {}s: {}\".format(\n",
        "                      start_time.seconds + start_time.microseconds * 1e-6,\n",
        "                      end_time.seconds + end_time.microseconds * 1e-6,\n",
        "                      word,\n",
        "                  )\n",
        "              )"
      ],
      "metadata": {
        "id": "V1Clb9Y2LPjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def send_video_intelligence_requests(video_name, gs_uri):\n",
        "  label_detection_api_request(video_name, gs_uri)\n",
        "  faces_detection_api_request(video_name, gs_uri)\n",
        "  shot_detection_api_request(video_name, gs_uri)\n",
        "  text_detection_api_request(video_name, gs_uri)\n",
        "  logo_detection_api_request(video_name, gs_uri)\n",
        "  speech_detection_api_request(video_name, gs_uri)"
      ],
      "metadata": {
        "id": "4QQg0lSKmmBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ABCD Detector\n",
        "\n",
        "## ABCD Signals detected for video"
      ],
      "metadata": {
        "id": "dCeQgYb4Hkt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1) Attract: Face Early\n",
        "#@markdown Is there a human face on screen in the 1st 5 seconds?\n",
        "#@markdown Definition: At least seen for 1s and with at least 5% of the surface.\n",
        "\n",
        "def is_face_early(video_name):\n",
        "  face_early = False\n",
        "  max_face_surface = 0\n",
        "  total_time_face_detected = 0\n",
        "\n",
        "  blob = output_bucket.blob(f\"facesdetection/facesdetection-{video_name}.json\")\n",
        "\n",
        "  # Download the contents of the blob as a string and then parse it using json.loads() method\n",
        "  data = json.loads(blob.download_as_string(client=None))\n",
        "\n",
        "  annotation_result = data.get(\"annotation_results\")[0]\n",
        "  if(annotation_result and annotation_result.get(\"face_detection_annotations\")):\n",
        "    for annotation in annotation_result.get(\"face_detection_annotations\"):\n",
        "      for track in annotation.get(\"tracks\"):\n",
        "        start_nanos = track.get(\"segment\").get(\"start_time_offset\").get(\"nanos\")\n",
        "        start_nanos_safe = start_nanos and start_nanos / 1e9 or 0\n",
        "        start_seconds_safe = track.get(\"segment\").get(\"start_time_offset\").get(\"seconds\") or 0\n",
        "        start_time = start_seconds_safe + start_nanos_safe\n",
        "        end_nanos = track.get(\"segment\").get(\"end_time_offset\").get(\"nanos\")\n",
        "        end_nanos_safe = end_nanos and end_nanos / 1e9 or 0\n",
        "        end_seconds_safe = track.get(\"segment\").get(\"end_time_offset\").get(\"seconds\") or 0\n",
        "        end_time = end_seconds_safe + end_nanos_safe\n",
        "\n",
        "        confidence = track.get(\"confidence\")\n",
        "\n",
        "        if(confidence >= confidence_threshold):\n",
        "          if(start_time < early_time_seconds):\n",
        "            # Each segment includes timestamped faces that include\n",
        "            # characteristics of the face detected.\n",
        "            face_objects = track.get(\"timestamped_objects\")\n",
        "            for i, face_object in enumerate(face_objects):\n",
        "              box = face_object.get(\"normalized_bounding_box\")\n",
        "              time_offset = face_object.get(\"time_offset\")\n",
        "              offset_nanos = time_offset.get(\"nanos\")\n",
        "              offset_nanos_safe = offset_nanos and offset_nanos / 1e9 or 0\n",
        "              offset_seconds_safe = time_offset.get(\"seconds\") or 0\n",
        "              start_seconds_safe = offset_seconds_safe + offset_nanos_safe\n",
        "\n",
        "              left = box.get(\"left\") or 0\n",
        "              right = box.get(\"right\") or 1\n",
        "              top = box.get(\"top\") or 0\n",
        "              bottom = box.get(\"bottom\") or 1\n",
        "\n",
        "              width = right - left\n",
        "              height = bottom - top\n",
        "              surface = width * height\n",
        "\n",
        "              next_offset_nanos_safe = 0\n",
        "              next_offset_seconds_safe = 0\n",
        "\n",
        "              next_i = i + 1\n",
        "              if(next_i < len(face_objects)):\n",
        "                next_box = face_objects[next_i].get(\"normalized_bounding_box\")\n",
        "                time_offset = face_objects[next_i].get(\"time_offset\")\n",
        "                offset_nanos = time_offset.get(\"nanos\")\n",
        "                next_offset_nanos_safe = offset_nanos and offset_nanos / 1e9 or 0\n",
        "                next_offset_seconds_safe = time_offset.get(\"seconds\") or 0\n",
        "\n",
        "              end_seconds_safe = next_offset_seconds_safe + next_offset_nanos_safe\n",
        "\n",
        "              if(end_seconds_safe > 0\n",
        "                and surface >= surface_threshold_percent):\n",
        "                max_face_surface = max(max_face_surface, surface)\n",
        "                total_time_face_detected += end_seconds_safe - start_seconds_safe\n",
        "\n",
        "            if(total_time_face_detected > duration_threshold_seconds):\n",
        "              face_early = True\n",
        "\n",
        "  return face_early"
      ],
      "metadata": {
        "id": "lZ5pVuVRwyuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2) Attract: Pacing quick\n",
        "#@markdown Is the pace of the video under 2 seconds per shot in the first 5 seconds?\n",
        "\n",
        "def is_pacing_quick(video_name):\n",
        "  pacing_quick = False\n",
        "  total_time_per_shot = 0\n",
        "  total_shots = 0\n",
        "\n",
        "  blob = output_bucket.blob(f\"shotdetection/shotdetection-{video_name}.json\")\n",
        "\n",
        "  # Download the contents of the blob as a string and then parse it using json.loads() method\n",
        "  data = json.loads(blob.download_as_string(client=None))\n",
        "\n",
        "  # first result is retrieved because a single video was processed\n",
        "  for i, shot in enumerate(data.get(\"annotation_results\")[0].get(\"shot_annotations\")):\n",
        "      start_time_microseconds = shot.get(\"start_time_offset\").get(\"microseconds\")\n",
        "      start_time_microseconds_safe = start_time_microseconds and start_time_microseconds / 1e6 or 0\n",
        "      end_time_microseconds = shot.get(\"end_time_offset\").get(\"microseconds\")\n",
        "      end_time_microseconds_safe = end_time_microseconds and end_time_microseconds / 1e6 or 0\n",
        "      start_time = (\n",
        "          shot.get(\"start_time_offset\").get(\"seconds\") or 0 + start_time_microseconds_safe\n",
        "      )\n",
        "      end_time = (\n",
        "          shot.get(\"end_time_offset\").get(\"seconds\") or 0 + end_time_microseconds_safe\n",
        "      )\n",
        "      total_shot_time = end_time - start_time\n",
        "      if(start_time < early_time_seconds):\n",
        "        total_time_per_shot += total_shot_time\n",
        "        total_shots += 1\n",
        "      #print(\"\\tShot {}: {} to {}: {} seconds\".format(i, start_time, end_time, total_shot_time))\n",
        "\n",
        "  # print(total_time_per_shot / total_shots)\n",
        "  if(total_time_per_shot / total_shots <= avg_shot_duration_seconds):\n",
        "    pacing_quick = True\n",
        "\n",
        "  return pacing_quick\n"
      ],
      "metadata": {
        "id": "DacJK1DO1FVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3) Attract: Dynamic Start\n",
        "#@markdown Does the shot changes in less than 3s?\n",
        "\n",
        "def has_dynamic_start(video_name):\n",
        "  dynamic_start = False\n",
        "  blob = output_bucket.blob(f\"shotdetection/shotdetection-{video_name}.json\")\n",
        "\n",
        "  # Download the contents of the blob as a string and then parse it using json.loads() method\n",
        "  data = json.loads(blob.download_as_string(client=None))\n",
        "\n",
        "  frame_start_ms = 0\n",
        "  end_time_off_set = data['annotation_results'][0]['shot_annotations'][0]\n",
        "  nanos = end_time_off_set.get(\"end_time_offset\").get(\"nanos\")\n",
        "  seconds = end_time_off_set.get(\"end_time_offset\").get(\"seconds\")\n",
        "  if(nanos):\n",
        "    if(seconds):\n",
        "      total_ms_first_shot = (nanos + seconds * 1e9) / 1e6\n",
        "    else:\n",
        "      total_ms_first_shot = nanos / 1e6\n",
        "  else:\n",
        "    if(seconds):\n",
        "      total_ms_first_shot = (seconds * 1e9) / 1e6\n",
        "\n",
        "\n",
        "  if total_ms_first_shot < dynamic_cutoff_ms:\n",
        "    dynamic_start = True\n",
        "\n",
        "  return dynamic_start\n"
      ],
      "metadata": {
        "id": "Rn6oD0iC64SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4 & 5) Brand: Logo Big & Logo Early\n",
        "#@markdown Is Logo larger than 3.5% of screen in the first 5 seconds?\n",
        "\n",
        "def is_logo_big_early(video_name):\n",
        "  brand_logo_big = False\n",
        "  brand_logo_early = False\n",
        "\n",
        "  blob = output_bucket.blob(f\"logodetection/logodetection-{video_name}.json\")\n",
        "  # Download the contents of the blob as a string and then parse it using json.loads() method\n",
        "  data = json.loads(blob.download_as_string(client=None))\n",
        "  annotation_result = data.get(\"annotation_results\")[0]\n",
        "\n",
        "  blob_text = output_bucket.blob(f\"textdetection/textdetection-{video_name}.json\")\n",
        "  # Download the contents of the blob as a string and then parse it using json.loads() method\n",
        "  data_text = json.loads(blob_text.download_as_string(client=None))\n",
        "  annotation_result_text = data_text.get(\"annotation_results\")[0]\n",
        "\n",
        "  for text_annotation in annotation_result_text.get(\"text_annotations\"):\n",
        "      if(brand_name in text_annotation.get(\"text\")):\n",
        "        frames = text_annotation.get(\"segments\")[0].get(\"frames\")\n",
        "        for frame in frames:\n",
        "          start_time_seconds = frame.get(\"time_offset\").get(\"seconds\") or 0\n",
        "          if(start_time_seconds <= early_time_seconds):\n",
        "            brand_logo_early = True\n",
        "            coordinates = []\n",
        "            for vertex in frame.get(\"rotated_bounding_box\").get(\"vertices\"):\n",
        "              coordinates.append(((float(vertex.get(\"x\"))), float(vertex.get(\"y\"))))\n",
        "            surface_area = calculate_surface_area(coordinates)\n",
        "            if(surface_area > logo_size_threshold):\n",
        "              brand_logo_big = True\n",
        "\n",
        "  # Annotations for list of logos detected, tracked and recognized in video.\n",
        "  for logo_recognition_annotation in annotation_result.get(\"logo_recognition_annotations\"):\n",
        "    entity = logo_recognition_annotation.get(\"entity\")\n",
        "    # print(f\"{entity}\")\n",
        "    if(entity.get(\"entity_id\") == entity_id or entity.get(\"description\") == entity_desc):\n",
        "      # All logo tracks where the recognized logo appears. Each track corresponds\n",
        "      # to one logo instance appearing in consecutive frames.\n",
        "      for track in logo_recognition_annotation.get(\"tracks\"):\n",
        "        confidence = track.get(\"confidence\")\n",
        "        # print(f\"Confidence: {confidence}\")\n",
        "        if(confidence > confidence_threshold):\n",
        "          # Video segment of a track.\n",
        "          seconds = track.get(\"segment\").get(\"start_time_offset\").get(\"seconds\")\n",
        "          # print(f\"Seen at: {seconds}s\")\n",
        "          if(seconds):\n",
        "            if(seconds <= early_time_seconds):\n",
        "              brand_logo_early = True\n",
        "              # The object with timestamp and attributes per frame in the track.\n",
        "              for timestamped_object in track.get(\"timestamped_objects\"):\n",
        "                # Normalized Bounding box in a frame, where the object is located.\n",
        "                normalized_bounding_box = timestamped_object.get(\"normalized_bounding_box\")\n",
        "                a = normalized_bounding_box.get(\"bottom\") - normalized_bounding_box.get(\"top\")\n",
        "                b = normalized_bounding_box.get(\"right\") - normalized_bounding_box.get(\"left\")\n",
        "                if (a * b * 100 > logo_size_threshold):\n",
        "                  brand_logo_big = True\n",
        "                  # print(f\"Surface is larger than 3.5%: {a*b*100}\")\n",
        "\n",
        "  return brand_logo_big, brand_logo_early\n",
        "\n",
        "def calculate_surface_area(points):\n",
        "  if(len(points) != 4):\n",
        "    return 0\n",
        "  area1 = 0.5 * abs(points[0][0] * points[1][1] - points[1][0] * points[0][1])\n",
        "  area2 = 0.5 * abs(points[1][0] * points[2][1] - points[2][0] * points[1][1])\n",
        "  area3 = 0.5 * abs(points[2][0] * points[3][1] - points[3][0] * points[2][1])\n",
        "  area4 = 0.5 * abs(points[3][0] * points[0][1] - points[0][0] * points[3][1])\n",
        "\n",
        "# Add the areas of the four triangles to get the total surface area.\n",
        "  surface_area = area1 + area2 + area3 + area4\n",
        "  return surface_area*100"
      ],
      "metadata": {
        "id": "TxoIUQr-87NC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 6) Brand: Product Early\n",
        "#@markdown Is the product visible in the first 5 seconds?\n",
        "\n",
        "def is_product_early(video_name):\n",
        "  brand_product_early = False\n",
        "\n",
        "  blob = output_bucket.blob(f\"labeldetection/labeldetection-{video_name}.json\")\n",
        "\n",
        "  # Download the contents of the blob as a string and then parse it using json.loads() method\n",
        "  data = json.loads(blob.download_as_string(client=None))\n",
        "\n",
        "  # Segments are user specified in a request to capture labels during a specific\n",
        "  # time frame. If unspecified, each video is treated as a single segment.\n",
        "  segment_labels = data.get(\"annotation_results\")[0].get(\n",
        "      \"segment_label_annotations\")\n",
        "\n",
        "  # Shots change each time a video cut occurs or the contents of the video have\n",
        "  # changed. When a new shot is detected, labels are annotated for the new shot.\n",
        "  shot_labels = data.get(\"annotation_results\")[0].get(\n",
        "      \"shot_label_annotations\")\n",
        "\n",
        "  for i, segment_label in enumerate(shot_labels):\n",
        "      segment_entity_id = segment_label.get(\"entity\").get(\"entity_id\")\n",
        "      segment_entity_description = segment_label.get(\"entity\").get(\"description\")\n",
        "      # print(segment_entity_description)\n",
        "      if(segment_entity_id == product_entity_id or segment_entity_description == product_entity_desc):\n",
        "        label_description = (\"Video label description: {}\".format(\n",
        "            segment_label.get(\"entity\").get(\"description\")))\n",
        "        for i, segment in enumerate(segment_label.get(\"segments\")):\n",
        "            confidence = segment.get(\"confidence\")\n",
        "            # print(f\"Confidence: {confidence}\")\n",
        "            if(confidence >= confidence_threshold):\n",
        "              start_nanos = segment.get(\"segment\").get(\n",
        "                  \"start_time_offset\").get(\n",
        "                      \"nanos\")\n",
        "              start_nanos_safe = start_nanos and start_nanos / 1e9 or 0\n",
        "              start_seconds_safe = segment.get(\"segment\").get(\"start_time_offset\").get(\"seconds\") or 0\n",
        "              start_time = start_seconds_safe + start_nanos_safe\n",
        "\n",
        "              \"\"\"end_nanos = segment.get(\"segment\").get(\n",
        "                  \"end_time_offset\").get(\n",
        "                      \"nanos\")\n",
        "              end_nanos_safe = end_nanos and end_nanos / 1e9 or 0\n",
        "              end_seconds_safe = segment.get(\"segment\").get(\"end_time_offset\").get(\"seconds\") or 0\n",
        "              end_time = end_seconds_safe + end_nanos_safe\"\"\"\n",
        "              if(start_time <= early_time_seconds):\n",
        "                brand_product_early = True\n",
        "                positions = \"{}s to {}s\".format(start_time, end_time)\n",
        "\n",
        "\n",
        "  return brand_product_early\n"
      ],
      "metadata": {
        "id": "OdsWCEXM9BLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 7) Brand: Name Early\n",
        "#@markdown Is there text on screen for the brand in the first 5 seconds?\n",
        "\n",
        "def is_name_early(video_name):\n",
        "  brand_name_early = False\n",
        "\n",
        "  blob = output_bucket.blob(f\"textdetection/textdetection-{video_name}.json\")\n",
        "  # Download the contents of the blob as a string and then parse it using json.loads() method\n",
        "  data = json.loads(blob.download_as_string(client=None))\n",
        "\n",
        "  # The first result is retrieved because a single video was processed.\n",
        "  annotation_result = data.get(\"annotation_results\")[0]\n",
        "\n",
        "\n",
        "  for text_annotation in annotation_result.get(\"text_annotations\"):\n",
        "      if(brand_name in text_annotation.get(\"text\")):\n",
        "        frames = text_annotation.get(\"segments\")[0].get(\"frames\")\n",
        "        for frame in frames:\n",
        "          start_time_seconds = frame.get(\"time_offset\").get(\"seconds\") or 0\n",
        "          if(start_time_seconds <= early_time_seconds):\n",
        "            brand_name_early = True\n",
        "\n",
        "\n",
        "  return brand_name_early"
      ],
      "metadata": {
        "id": "PnQZJowo9F7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 8) Connect: Face Close\n",
        "#@markdown Is there a close up of the human face?\n",
        "\n",
        "def is_face_close(video_name):\n",
        "  connect_face_close = False\n",
        "\n",
        "  max_face_surface = 0\n",
        "  face_coverage = 0\n",
        "  total_time_face_detected = 0\n",
        "\n",
        "  blob = output_bucket.blob(f\"facesdetection/facesdetection-{video_name}.json\")\n",
        "  # Download the contents of the blob as a string and then parse it using json.loads() method\n",
        "  data = json.loads(blob.download_as_string(client=None))\n",
        "  annotation_result = data.get(\"annotation_results\")[0]\n",
        "\n",
        "  video_length_seconds_safe = annotation_result.get(\"segment\").get(\"end_time_offset\").get(\"seconds\") or 0\n",
        "  video_length_nanos_safe = annotation_result.get(\"segment\").get(\"end_time_offset\").get(\"nanos\") / 1e9 or 0\n",
        "  video_length = video_length_seconds_safe + video_length_nanos_safe\n",
        "\n",
        "  # print(f\"Video Length: {video_length}s\")\n",
        "\n",
        "  if(annotation_result.get(\"face_detection_annotations\")):\n",
        "    for annotation in annotation_result.get(\"face_detection_annotations\"):\n",
        "      for track in annotation.get(\"tracks\"):\n",
        "        start_nanos = track.get(\"segment\").get(\"start_time_offset\").get(\"nanos\")\n",
        "        start_nanos_safe = start_nanos and start_nanos / 1e9 or 0\n",
        "        start_seconds_safe = track.get(\"segment\").get(\"start_time_offset\").get(\"seconds\") or 0\n",
        "        start_time = start_seconds_safe + start_nanos_safe\n",
        "        end_nanos = track.get(\"segment\").get(\"end_time_offset\").get(\"nanos\")\n",
        "        end_nanos_safe = end_nanos and end_nanos / 1e9 or 0\n",
        "        end_seconds_safe = track.get(\"segment\").get(\"end_time_offset\").get(\"seconds\") or 0\n",
        "        end_time = end_seconds_safe + end_nanos_safe\n",
        "        face_coverage += (end_time - start_time)\n",
        "\n",
        "        if(track.get(\"confidence\") >= confidence_threshold):\n",
        "          # print(track.get(\"confidence\"))\n",
        "          # Each segment includes timestamped faces that include\n",
        "          # characteristics of the face detected.\n",
        "          face_objects = track.get(\"timestamped_objects\")\n",
        "          for i, face_object in enumerate(face_objects):\n",
        "            box = face_object.get(\"normalized_bounding_box\")\n",
        "            time_offset = face_object.get(\"time_offset\")\n",
        "            # print(time_offset)\n",
        "            offset_nanos = time_offset.get(\"nanos\")\n",
        "            offset_nanos_safe = offset_nanos and offset_nanos / 1e9 or 0\n",
        "            offset_seconds_safe = time_offset.get(\"seconds\") or 0\n",
        "            box_start_seconds_safe = offset_seconds_safe + offset_nanos_safe\n",
        "\n",
        "            left = box.get(\"left\") or 0\n",
        "            right = box.get(\"right\") or 1\n",
        "            top = box.get(\"top\") or 0\n",
        "            bottom = box.get(\"bottom\") or 1\n",
        "\n",
        "            width = right - left\n",
        "            height = bottom - top\n",
        "            surface = width * height\n",
        "\n",
        "            next_offset_nanos_safe = 0\n",
        "            next_offset_seconds_safe = 0\n",
        "\n",
        "            next_i = i + 1\n",
        "            if(next_i < len(face_objects)):\n",
        "              next_box = face_objects[next_i].get(\"normalized_bounding_box\")\n",
        "              time_offset = face_objects[next_i].get(\"time_offset\")\n",
        "              offset_nanos = time_offset.get(\"nanos\")\n",
        "              next_offset_nanos_safe = offset_nanos and offset_nanos / 1e9 or 0\n",
        "              next_offset_seconds_safe = time_offset.get(\"seconds\") or 0\n",
        "\n",
        "            box_end_seconds_safe = next_offset_seconds_safe + next_offset_nanos_safe\n",
        "            # print(f\"surface: {surface} vs limit: {surface_threshold}\")\n",
        "            if(box_end_seconds_safe > 0 and surface >= face_surface_threshold):\n",
        "              max_face_surface = max(max_face_surface, surface)\n",
        "              total_time_face_detected += box_end_seconds_safe - box_start_seconds_safe\n",
        "  # print(f\"Total closeup face time detected : {total_time_face_detected}s\")\n",
        "\n",
        "  if(total_time_face_detected > face_duration_threshold_seconds):\n",
        "    connect_face_close = True\n",
        "\n",
        "  return connect_face_close"
      ],
      "metadata": {
        "id": "8qaosLHbye0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 9) Connect: Overall Pacing\n",
        "#@markdown Is the pace of video greater than 2 seconds per shot?\n",
        "\n",
        "def is_overall_pacing(video_name):\n",
        "  overall_pacing = False\n",
        "  total_time_per_shot = 0\n",
        "  total_shots = 0\n",
        "\n",
        "  blob = output_bucket.blob(f\"shotdetection/shotdetection-{video_name}.json\")\n",
        "\n",
        "  # Download the contents of the blob as a string and then parse it using json.loads() method\n",
        "  data = json.loads(blob.download_as_string(client=None))\n",
        "\n",
        "  # first result is retrieved because a single video was processed\n",
        "  for i, shot in enumerate(data.get(\"annotation_results\")[0].get(\"shot_annotations\")):\n",
        "      # print(shot)\n",
        "      start_time_microseconds = shot.get(\"start_time_offset\").get(\"microseconds\")\n",
        "      start_time_microseconds_safe = start_time_microseconds and start_time_microseconds / 1e6 or 0\n",
        "      end_time_microseconds = shot.get(\"end_time_offset\").get(\"microseconds\")\n",
        "      end_time_microseconds_safe = end_time_microseconds and end_time_microseconds / 1e6 or 0\n",
        "      start_time = (\n",
        "          shot.get(\"start_time_offset\").get(\"seconds\") or 0 + start_time_microseconds_safe\n",
        "      )\n",
        "      end_time = (\n",
        "          shot.get(\"end_time_offset\").get(\"seconds\") or 0 + end_time_microseconds_safe\n",
        "      )\n",
        "      total_shot_time = end_time - start_time\n",
        "      total_time_per_shot += total_shot_time\n",
        "      total_shots += 1\n",
        "      # print(\"\\tShot {}: {} to {}: {} seconds\".format(i, start_time, end_time, total_shot_time))\n",
        "\n",
        "  # print(total_time_per_shot / total_shots)\n",
        "  if(total_time_per_shot / total_shots <= overall_shot_pace_threshold):\n",
        "    overall_pacing = True\n",
        "\n",
        "  return overall_pacing\n"
      ],
      "metadata": {
        "id": "VDiwqSj-yeqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 10) Direct: Audio Early\n",
        "#@markdown Is speech detected in the audio in the first 5 seconds?\n",
        "def is_audio_early(video_name):\n",
        "  direct_audio_early = False\n",
        "  blob = output_bucket.blob(f\"transcribe/transcribe-{video_name}.json\")\n",
        "\n",
        "  # Download the contents of the blob as a string and then parse it using json.loads() method\n",
        "  data = json.loads(blob.download_as_string(client=None))\n",
        "  annotation_results = data.get(\"annotation_results\")[0]\n",
        "\n",
        "  speech_transcriptions = annotation_results.get(\"speech_transcriptions\")\n",
        "  if(speech_transcriptions):\n",
        "    for speech_transcription in annotation_results.get(\"speech_transcriptions\"):\n",
        "        for alternative in speech_transcription.get(\"alternatives\"):\n",
        "          if(alternative.get(\"confidence\")):\n",
        "            if(alternative.get(\"confidence\") >= confidence_threshold):\n",
        "              if(alternative.get(\"words\")):\n",
        "                for word_info in alternative.get(\"words\"):\n",
        "                    word = word_info.get(\"word\")\n",
        "                    start_time = word_info.get(\"start_time\")\n",
        "                    start_time_seconds = start_time.get(\"seconds\") or 0\n",
        "                    if(start_time_seconds <= early_time_seconds):\n",
        "                      direct_audio_early = True\n",
        "\n",
        "  return direct_audio_early\n"
      ],
      "metadata": {
        "id": "Ga6loEL6yeg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 11) Direct: Call To Action\n",
        "#@markdown Is there a call to action phrase detected on the video in the speech or text?\n",
        "\n",
        "def is_call_to_action(video_name):\n",
        "  model = TextGenerationModel.from_pretrained(\"text-bison\")\n",
        "\n",
        "  # https://developers.google.com/google-ads/api/rest/reference/rest/latest/CallToActionType?hl=en\n",
        "  call_to_action_text_api_list = ['LEARN MORE','GET QUOTE', 'APPLY NOW', 'SIGN UP', 'CONTACT US', 'SUBSCRIBE', 'DOWNLOAD', 'BOOK NOW', 'SHOP NOW', 'BUY NOW', 'DONATE NOW', 'ORDER NOW', 'PLAY NOW', 'SEE MORE', 'START NOW', 'VISIT SITE', 'WATCH NOW']\n",
        "  call_to_action_text_api_list_lower = [x.lower() for x in call_to_action_text_api_list]\n",
        "  call_to_action_text_verbs_list = ['LEARN','QUOTE', 'APPLY', 'SIGN UP', 'CONTACT', 'SUBSCRIBE', 'DOWNLOAD', 'BOOK', 'SHOP', 'BUY', 'DONATE', 'ORDER', 'PLAY', 'SEE', 'START', 'VISIT', 'WATCH']\n",
        "  call_to_action_text_verbs_list_lower = [x.lower() for x in call_to_action_text_verbs_list]\n",
        "\n",
        "  call_to_action_early = False\n",
        "  call_to_action_speech = False\n",
        "  call_to_action_text = False\n",
        "\n",
        "  call_to_action_speech = detect_call_to_action_speech(video_name, model, call_to_action_text_verbs_list_lower)\n",
        "  call_to_action_text = detect_call_to_action_text(video_name, model, call_to_action_text_verbs_list_lower)\n",
        "\n",
        "  return call_to_action_speech, call_to_action_text\n",
        "\n",
        "def prompt_llm_cta(model, transcript):\n",
        "  parameters = {\n",
        "      \"temperature\": 0.2,\n",
        "      \"max_output_tokens\": 1,\n",
        "      \"top_p\": 0.8,\n",
        "      \"top_k\": 40\n",
        "      }\n",
        "  #todo: how would this work for different languages.\n",
        "  prompt = 'Is there a call to action in my ad: \"' + transcript + '\"? If the transcript is not in English, first translate it to English then do the check for call to action. Show the English version of the text too.'\n",
        "  print('prompt: ', prompt)\n",
        "\n",
        "  model_output = model.predict(prompt, **parameters).text\n",
        "\n",
        "  time.sleep(1) #in order not to exceed text-bison quota of 60 requests\n",
        "  # per minute (quota increase could be requested)\n",
        "\n",
        "  print('llm said:', model_output)\n",
        "  if(('yes' in model_output.lower())):\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def detect_call_to_action_speech(video_name, model, cta_list):\n",
        "  blob = output_bucket.blob(f\"transcribe/transcribe-{video_name}.json\")\n",
        "\n",
        "  # Download the contents of the blob as a string and then parse it using json.loads() method\n",
        "  data = json.loads(blob.download_as_string(client=None))\n",
        "  annotation_results = data.get(\"annotation_results\")[0]\n",
        "\n",
        "  speech_transcriptions = annotation_results.get(\"speech_transcriptions\")\n",
        "  if(speech_transcriptions):\n",
        "    for speech_transcription in speech_transcriptions:\n",
        "        for alternative in speech_transcription.get(\"alternatives\"):\n",
        "          transcript = alternative.get(\"transcript\")\n",
        "          if(alternative.get(\"confidence\")):\n",
        "            if(alternative.get(\"confidence\") >= confidence_threshold):\n",
        "              for word in transcript.split():\n",
        "                # print(word)\n",
        "                for cta in cta_list:\n",
        "                  if(word.lower() == cta):\n",
        "                    return True\n",
        "              # if(prompt_llm_cta(model, transcript)):\n",
        "                # return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "\n",
        "def detect_call_to_action_text(video_name, model, cta_list):\n",
        "  blob_text = output_bucket.blob(f\"textdetection/textdetection-{video_name}.json\")\n",
        "  # Download the contents of the blob as a string and then parse it using json.loads() method\n",
        "  data_text = json.loads(blob_text.download_as_string(client=None))\n",
        "  annotation_result_text = data_text.get(\"annotation_results\")[0]\n",
        "\n",
        "  # print(\"Text on screen: \")\n",
        "  for text_annotation in annotation_result_text.get(\"text_annotations\"):\n",
        "    words = text_annotation.get(\"text\")\n",
        "    for word in words.split():\n",
        "      for cta in cta_list:\n",
        "        if(word.lower() == cta):\n",
        "          # print(word)\n",
        "          return True\n",
        "    #if(prompt_llm_cta(model, words)):\n",
        "    # return True\n",
        "  return False\n"
      ],
      "metadata": {
        "id": "zJrfgKXHyeV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def colored(r, g, b, text):\n",
        "    return f\"\\033[38;2;{r};{g};{b}m{text}\\033[0m\"\n",
        "\n",
        "def print_red(text):\n",
        "  print(colored(255, 0, 0, text))\n",
        "\n",
        "def print_green(text):\n",
        "  print(colored(0, 128, 0, text))\n",
        "\n",
        "def print_boolean(bool_flag, text):\n",
        "  if(bool_flag):\n",
        "    print_green(text)\n",
        "  else:\n",
        "    print_red(text)\n"
      ],
      "metadata": {
        "id": "_LUKHE74OYvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_json_file(json_object, filename):\n",
        "    blob = output_bucket.blob(filename)\n",
        "    blob.upload_from_string(\n",
        "        data=json_object,\n",
        "        content_type='application/json'\n",
        "        )\n",
        "    result = filename + ' upload complete'\n",
        "    print(result)"
      ],
      "metadata": {
        "id": "P8Ku9p1_tVnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_jsonl_data(json_object):\n",
        "    data = \"\"\n",
        "    for entry in json_object:\n",
        "      data+=json.dumps(entry)\n",
        "      data+=\"\\n\"\n",
        "    return data"
      ],
      "metadata": {
        "id": "E-83gsvNDR9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_abcds(video_name):\n",
        "  score = 0\n",
        "\n",
        "  abcd_names = [\n",
        "                \"attract_face_early\",\n",
        "                \"attract_pacing_quick\",\n",
        "                \"attract_dynamic_start\",\n",
        "                \"brand_logo_big\",\n",
        "                \"brand_logo_early\",\n",
        "                \"brand_product_early\",\n",
        "                \"brand_name_early\",\n",
        "                \"connect_face_close\",\n",
        "                \"connect_overall_pacing\",\n",
        "                \"direct_audio_early\",\n",
        "                \"direct_cta_speech\",\n",
        "                \"direct_cta_text\"]\n",
        "\n",
        "  abcd_values = [\n",
        "      is_face_early(video_name),\n",
        "      is_pacing_quick(video_name),\n",
        "      has_dynamic_start(video_name),\n",
        "      list(is_logo_big_early(video_name))[0],\n",
        "      list(is_logo_big_early(video_name))[1],\n",
        "      is_product_early(video_name),\n",
        "      is_name_early(video_name),\n",
        "      is_face_close(video_name),\n",
        "      is_overall_pacing(video_name),\n",
        "      is_audio_early(video_name),\n",
        "      list(is_call_to_action(video_name))[0],\n",
        "      list(is_call_to_action(video_name))[1]]\n",
        "\n",
        "  abcd_dict = dict(zip(abcd_names, abcd_values))\n",
        "\n",
        "  for abcd in abcd_values:\n",
        "    if(abcd):\n",
        "      score += 1\n",
        "\n",
        "  abcd_dict[\"video_name\"] = video_name\n",
        "  abcd_dict[\"score\"] = score/12*100\n",
        "\n",
        "  print(f\"Video: {abcd_dict.get('video_name')}\")\n",
        "  print(f\"Total score: {abcd_dict.get('score')}%\")\n",
        "  print_boolean(\n",
        "      abcd_dict.get('attract_face_early'),\n",
        "      f\"1) Attract: Face Early                      : {abcd_dict.get('attract_face_early')}\")\n",
        "  print_boolean(\n",
        "      abcd_dict.get('attract_pacing_quick'),\n",
        "      f\"2) Attract: Pace Quick                      : {abcd_dict.get('attract_pacing_quick')}\")\n",
        "  print_boolean(\n",
        "      abcd_dict.get('attract_dynamic_start'),\n",
        "      f\"3) Attract: Dynamic Start                   : {abcd_dict.get('attract_dynamic_start')}\")\n",
        "  print_boolean(\n",
        "      abcd_dict.get('brand_logo_big'),\n",
        "      f\"4) Brand: Logo Big                          : {abcd_dict.get('brand_logo_big')}\")\n",
        "  print_boolean(\n",
        "      abcd_dict.get('brand_logo_early'),\n",
        "      f\"5) Brand: Logo Early                        : {abcd_dict.get('brand_logo_early')}\")\n",
        "  print_boolean(\n",
        "      abcd_dict.get('brand_product_early'),\n",
        "      f\"6) Brand: Product Early                     : {abcd_dict.get('brand_product_early')}\")\n",
        "  print_boolean(\n",
        "      abcd_dict.get('brand_name_early'),\n",
        "      f\"7) Brand: Name Early                        : {abcd_dict.get('brand_name_early')}\")\n",
        "  print_boolean(\n",
        "      abcd_dict.get('connect_face_close'),\n",
        "      f\"8) Connect: Face Close                      : {abcd_dict.get('connect_face_close')}\")\n",
        "  print_boolean(\n",
        "      abcd_dict.get('connect_overall_pacing'),\n",
        "      f\"9) Connect: Overall Pacing                  : {abcd_dict.get('connect_overall_pacing')}\")\n",
        "  print_boolean(\n",
        "      abcd_dict.get('direct_audio_early'),\n",
        "      f\"10) Direct: Audio Early                     : {abcd_dict.get('direct_audio_early')}\")\n",
        "  print_boolean(\n",
        "      abcd_dict.get('direct_cta_speech'),\n",
        "      f\"11a) Direct: Call to Action (speech)        : {abcd_dict.get('direct_cta_speech')}\")\n",
        "  print_boolean(\n",
        "      abcd_dict.get('direct_cta_text'),\n",
        "      f\"11b) Direct: Call to Action (text on screen): {abcd_dict.get('direct_cta_text')}\")\n",
        "\n",
        "  return abcd_dict"
      ],
      "metadata": {
        "id": "BhCTXH_1kToR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Summary ABCD\n",
        "videos = storage_client.list_blobs(input_bucket_name)\n",
        "abcd_jsons = []\n",
        "\n",
        "# Note: The call returns a response only when the iterator is consumed.\n",
        "for video in videos:\n",
        "  video_name = video.name\n",
        "  gs_uri = \"gs://\" + input_bucket_name + \"/\" + video_name\n",
        "  video_url = '/content/' + input_bucket_name + '/' + video_name\n",
        "\n",
        "  send_video_intelligence_requests(video_name, gs_uri)\n",
        "  result = detect_abcds(video_name)\n",
        "  abcd_jsons.append(result)\n",
        "\n",
        "\n",
        "abcd_jsonls = get_jsonl_data(abcd_jsons)\n",
        "filename = \"final_abcd_report.json\"\n",
        "save_json_file(abcd_jsonls, filename)"
      ],
      "metadata": {
        "id": "zj7Ci9lyeFXp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
